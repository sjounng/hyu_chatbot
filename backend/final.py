import os
import json
import pickle
import random
from collections import OrderedDict
from langchain_community.vectorstores import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from typing import List
from dotenv import load_dotenv
from sentence_transformers.cross_encoder import CrossEncoder

# --- 0. ì„¤ì • ë° ì´ˆê¸°í™” ---
# [ìˆ˜ì •] content í´ë”ë¥¼ ê¸°ë³¸ ê²½ë¡œë¡œ ì„¤ì •í•˜ê³ , op.env íŒŒì¼ì˜ ì •í™•í•œ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
BASE_DIR = "content"
ENV_PATH = os.path.join(BASE_DIR, "op.env")

# [ìˆ˜ì •] op.env íŒŒì¼ì„ ëª…ì‹œì ìœ¼ë¡œ ë¡œë“œ
print(f"'{ENV_PATH}'ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
load_dotenv(dotenv_path=ENV_PATH)

API_KEY = os.getenv("OPENAI_API_KEY")

# API í‚¤ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
if not API_KEY:
    raise ValueError(f"OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. '{ENV_PATH}' íŒŒì¼ì— í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

# [ìˆ˜ì •] ëª¨ë¸ ë° ëª¨ë“  íŒŒì¼ ê²½ë¡œì— BASE_DIRë¥¼ ì ìš©í•©ë‹ˆë‹¤.
CHAT_MODEL = 'gpt-4o'
VECTOR_DB_PATH = os.path.join(BASE_DIR, "chroma_db_hyu")
BM25_INDEX_PATH = os.path.join(BASE_DIR, "bm25_index.pkl")
DOC_FILE_PATH = os.path.join(BASE_DIR, "hyuwiki_documents_20250621_234549.json")
QA_FILE_PATH = os.path.join(BASE_DIR, "qa_random_200_samples_20250622_203907.json")


# LLM ì´ˆê¸°í™” (ê²°ì •ë¡ ì  ë‹µë³€ì„ ìœ„í•´ temperature=0.0 ì„¤ì •)
llm_final = ChatOpenAI(model_name=CHAT_MODEL, temperature=0.0, openai_api_key=API_KEY)

# --- 1. ê²€ìƒ‰ ì‹œìŠ¤í…œ ë¡œë“œ ---
print("1. ê²€ìƒ‰ ì‹œìŠ¤í…œ ë° ì „ì²´ ë¬¸ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
embedding_model = OpenAIEmbeddings(openai_api_key=API_KEY)
vector_retriever = Chroma(
    persist_directory=VECTOR_DB_PATH, embedding_function=embedding_model
).as_retriever(search_kwargs={"k": 20})

with open(BM25_INDEX_PATH, "rb") as f:
    bm25_retriever = pickle.load(f)
    bm25_retriever.k = 20

with open(DOC_FILE_PATH, "r", encoding="utf-8") as f:
    all_docs_data = json.load(f)
title_to_doc_map = {item["title"]: Document(page_content=item["content"], metadata=item) for item in all_docs_data}
all_titles = list(title_to_doc_map.keys())

# CrossEncoder ëª¨ë¸ ë¡œë“œ
cross_encoder = CrossEncoder('bongsoo/kpf-cross-encoder-v1')
print("   -> ë¡œë“œ ì™„ë£Œ!")

# --- ìƒˆë¡œìš´ í•¨ìˆ˜ (Re-ranker) ---
def rerank_documents(query: str, documents: List[Document]) -> List[Document]:
    """CrossEncoderë¥¼ ì‚¬ìš©í•´ ë¬¸ì„œë“¤ì„ ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì¬ì •ë ¬í•©ë‹ˆë‹¤."""
    if not documents:
        return []
    print(f"   -> CrossEncoderë¡œ {len(documents)}ê°œ ë¬¸ì„œ ì¬ìˆœìœ„í™” ì§„í–‰...")
    pairs = [[query, doc.page_content] for doc in documents]
    scores = cross_encoder.predict(pairs, show_progress_bar=False)

    doc_with_scores = list(zip(scores, documents))
    doc_with_scores.sort(key=lambda x: x[0], reverse=True)

    reranked_docs = [doc for score, doc in doc_with_scores]
    print("   -> ì¬ìˆœìœ„í™” ì™„ë£Œ!")
    return reranked_docs

# --- 3. ìµœì¢… RAG ë‹µë³€ ìƒì„± í•¨ìˆ˜ (ì¶”ì¶œ ê°•í™” ë° BM25 ë””ë²„ê¹… ì¶”ê°€ ë²„ì „) ---
def get_final_response(query: str):
    """ë‹¤ì¤‘ ê²€ìƒ‰, RRF, Re-ranking í›„, ì‚¬ì‹¤ ê¸°ë°˜ ì¶”ì¶œì  ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""

    print("\n[ë‹¨ê³„ 1: ì´ˆê³ ì† ë™ì‹œ ê²€ìƒ‰ (ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©)]")

    # ê²€ìƒ‰ A: í™•ì •ì  Title ê²€ìƒ‰
    golden_docs = []
    query_no_space = query.replace(" ", "")
    for title in all_titles:
        title_no_space = title.replace(" ", "")
        if title in query or title_no_space in query_no_space:
            golden_docs.append(title_to_doc_map[title])
    if golden_docs:
        print(f"   -> 'ê³¨ë“  í‹°ì¼“' ë°œê²¬: {[doc.metadata['title'] for doc in golden_docs]}")

    # ê²€ìƒ‰ B: BM25 í‚¤ì›Œë“œ ê²€ìƒ‰
    bm25_docs = bm25_retriever.invoke(query)
    print(f"   -> BM25 ê²€ìƒ‰ìœ¼ë¡œ {len(bm25_docs)}ê°œì˜ í›„ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    # â˜…â˜…â˜… ì´ ë¶€ë¶„ì´ ì¶”ê°€ëœ ë””ë²„ê¹… ë¡œê·¸ì…ë‹ˆë‹¤. â˜…â˜…â˜…
    print("[BM25 ê²€ìƒ‰ ê²°ê³¼ í™•ì¸ (ë””ë²„ê¹…ìš©)]")
    if bm25_docs:
        for i, doc in enumerate(bm25_docs):
            print(f"  {i+1}. [ì¶œì²˜: {doc.metadata.get('title')}] {doc.page_content[:150]}...")
    else:
        print("  -> BM25ë¡œ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    print("-" * 20)
    # â˜…â˜…â˜… ì—¬ê¸°ê¹Œì§€ê°€ ì¶”ê°€ëœ ë¶€ë¶„ì…ë‹ˆë‹¤. â˜…â˜…â˜…

    # ê²€ìƒ‰ C: ë²¡í„° ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
    vector_docs = vector_retriever.invoke(query)
    print(f"   -> ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ {len(vector_docs)}ê°œì˜ í›„ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    # [ë‹¨ê³„ 2: RRF 1ì°¨ ìœµí•© ë° CrossEncoder ì¬ìˆœìœ„í™”]
    print("\n[ë‹¨ê³„ 2: RRF ìœµí•© ë° CrossEncoder ì¬ìˆœìœ„í™”]")
    rrf_scores = {}
    all_search_results = [golden_docs, bm25_docs, vector_docs]
    for results in all_search_results:
        for i, doc in enumerate(results):
            # doc.metadata['id']ê°€ ì—†ëŠ” ê²½ìš°ì— ëŒ€í•œ ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€
            doc_id = doc.metadata.get('id', doc.page_content)
            if doc_id not in rrf_scores:
                rrf_scores[doc_id] = {'score': 0, 'doc': doc}
            rrf_scores[doc_id]['score'] += 1.0 / (i + 60)

    sorted_docs_with_scores = sorted(rrf_scores.values(), key=lambda x: x['score'], reverse=True)
    fused_docs = [item['doc'] for item in sorted_docs_with_scores][:10]

    # CrossEncoder ì¬ìˆœìœ„í™”
    final_retrieved_docs = rerank_documents(query, fused_docs)[:5]

    if not final_retrieved_docs:
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []

    print("\n[ìµœì¢… ì„ ë³„ëœ ë¬¸ì„œ (LLM ì „ë‹¬ìš©)]")
    for i, doc in enumerate(final_retrieved_docs):
        print(f"  {i+1}. [ì¶œì²˜: {doc.metadata.get('title')}]")
    print("-" * 20)

    # ... (ì´í•˜ ë‚˜ë¨¸ì§€ ë‹µë³€ ìƒì„± ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼í•©ë‹ˆë‹¤) ...
    context_str = "\n\n---\n\n".join([f"ë¬¸ì„œ ì œëª©: {doc.metadata.get('title')}\në‚´ìš©: {doc.page_content}" for doc in final_retrieved_docs])
    source_info = [doc.metadata for doc in final_retrieved_docs]

    rag_prompt = f"""[ì§€ì‹œ]
ë‹¹ì‹ ì€ ì£¼ì–´ì§„ [ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©]ì—ì„œ ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì°¾ì•„ë‚´ì–´ ìš”ì•½í•˜ëŠ” 'ì •ë³´ ë¶„ì„ê°€'ì…ë‹ˆë‹¤.

[ê·œì¹™]
1.  **ë°˜ë“œì‹œ** [ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©]ì— ëª…ì‹œëœ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
2.  ì ˆëŒ€ë¡œ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ë¡ , ê°€ì •, ë˜ëŠ” ë³´ì¶©í•˜ì—¬ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”.
3.  ë‹µë³€ì€ [ì§ˆë¬¸]ì— ëŒ€í•œ í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ê³ , ê´€ë ¨ëœ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš©í•˜ëŠ” í˜•íƒœë¡œ êµ¬ì„±í•˜ì„¸ìš”.
4.  ì •ë³´ë¥¼ ê°€ì ¸ì˜¨ ê° ë¬¸ì¥ì´ë‚˜ ë‹¨ë½ ëì— `[ì¶œì²˜: ë¬¸ì„œ ì œëª©]` í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê·¼ê±°ë¥¼ ëª…í™•í•˜ê²Œ ë°í˜€ì•¼ í•©ë‹ˆë‹¤.
5.  ì—¬ëŸ¬ ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ìˆë‹¤ë©´, ì¢…í•©í•˜ë˜ ê° ì •ë³´ì˜ ì¶œì²˜ë¥¼ ì •í™•íˆ í‘œê¸°í•˜ì„¸ìš”.
6.  ë§Œì•½ [ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©]ì— [ì§ˆë¬¸]ê³¼ ê´€ë ¨ëœ ì •ë³´ê°€ ì—†ë‹¤ë©´, "ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.

---
[ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©]
{context_str}
---
[ì§ˆë¬¸]
{query}
[ë‹µë³€]"""

    print(f"\n[ë‹¨ê³„ 3: {CHAT_MODEL} ëª¨ë¸ë¡œ ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ ìƒì„± (temperature=0.0)]")
    try:
        answer = llm_final.invoke(rag_prompt).content.strip()
        verification_prompt = f"""[ì§€ì‹œ]
ì•„ë˜ [ìƒì„±ëœ ë‹µë³€]ì˜ ëª¨ë“  ë¬¸ì¥ì´ [ì›ë³¸ ë¬¸ì„œ]ì— ê·¼ê±°í•˜ê³  ìˆëŠ”ì§€ ê²€ì¦í•˜ì„¸ìš”.
ê·¼ê±°ê°€ ì—†ëŠ” ë¬¸ì¥ì´ë‚˜ ì¶”ë¡ ì´ í¬í•¨ëœ ë¬¸ì¥ì€ ì‚­ì œí•˜ê±°ë‚˜, [ì›ë³¸ ë¬¸ì„œ]ì˜ í‘œí˜„ìœ¼ë¡œ ìˆ˜ì •í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ë§Œë“œì„¸ìš”.

[ì›ë³¸ ë¬¸ì„œ]
{context_str}
---
[ìƒì„±ëœ ë‹µë³€]
{answer}
---
[ê²€ì¦ ì™„ë£Œëœ ìµœì¢… ë‹µë³€]"""
        print("\n[ë‹¨ê³„ 4: ìƒì„±ëœ ë‹µë³€ì— ëŒ€í•œ ìì²´ ê²€ì¦ ìˆ˜í–‰]")
        verified_answer = llm_final.invoke(verification_prompt).content.strip()
        return verified_answer, source_info
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", []

# --- 4. ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
if __name__ == "__main__":
    print("\nì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'ì¢…ë£Œ' ì…ë ¥)")
    while True:
        user_query = input("\nğŸ¤” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
        if user_query.lower() in ['exit', 'ì¢…ë£Œ']:
            print("ğŸ¤– ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."); break
        answer, sources = get_final_response(user_query)
        print("\n" + "="*50)
        print(f"ğŸ¤– ë‹µë³€:\n{answer}")
        if sources:
            # ë‹µë³€ì— ì´ë¯¸ ì¶œì²˜ê°€ í¬í•¨ë˜ì–´ ìˆì§€ë§Œ, ì°¸ê³  ìë£Œ ëª©ë¡ì„ ë³„ë„ë¡œ ì œê³µ
            unique_sources = list(OrderedDict.fromkeys((src.get('title'), src.get('url')) for src in sources))
            print("\nğŸ“š ì°¸ê³  ìë£Œ:")
            for title, url in unique_sources:
                print(f"  - {title} ({url})")
        print("="*50)
