import os
import json
import pickle
from collections import OrderedDict
from typing import List, Set, Dict, Any, Optional
from datetime import datetime
import uuid
from dotenv import load_dotenv

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from contextlib import asynccontextmanager

from langchain_community.vectorstores import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from sentence_transformers.cross_encoder import CrossEncoder

# --- FastAPI ëª¨ë¸ ì •ì˜ ---
class ChatRequest(BaseModel):
    message: str
    conversation_id: Optional[str] = None

class Source(BaseModel):
    id: str
    title: str
    content: str

class ChatResponse(BaseModel):
    response: str
    conversation_id: str
    timestamp: str
    sources: List[Source] = []

# --- ì„¤ì • ---
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
env_paths = [
    ".env", 
    "o.env",
    "../.env",
    os.path.join(os.path.dirname(__file__), ".env"),
    os.path.join(os.path.dirname(__file__), "o.env")
]

for env_path in env_paths:
    if os.path.exists(env_path):
        load_dotenv(dotenv_path=env_path)
        print(f"í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ: {env_path}")
        break
else:
    load_dotenv()
    print("ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©")

API_KEY = os.getenv("OPENAI_API_KEY")
if not API_KEY:
    raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

CHAT_MODEL = "gpt-4o"

# ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •
DATA_DIR = os.path.join(os.path.dirname(__file__), "data")
VECTOR_DB_PATH = os.path.join(DATA_DIR, "chroma_db_hyu")
BM25_INDEX_PATH = os.path.join(DATA_DIR, "bm25_index.pkl")
DOC_FILE_PATH = os.path.join(DATA_DIR, "document.json")
MATCHED_JSON_PATH = os.path.join(DATA_DIR, "matched.json")
FEW_SHOT_SAMPLES_PATH = os.path.join(DATA_DIR, "question_sample.json")

# --- í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
SYSTEM_PROMPT_GAP_ANALYSIS = """[ì§€ì‹œ]
ë‹¹ì‹ ì€ 'ì •ë³´ ë¶„ì„ê°€'ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ [ë¬¸ì„œë“¤]ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”.
1. ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ì— ëŒ€í•´ í˜„ì¬ê¹Œì§€ì˜ ì •ë³´ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
2. í˜„ì¬ ë¬¸ì„œë“¤ë§Œìœ¼ë¡œëŠ” ë‹µë³€ì´ ë¶ˆì™„ì „í•  ê²½ìš°, ë¶€ì¡±í•œ ì •ë³´ê°€ ë¬´ì—‡ì¸ì§€ ê°„ëµíˆ ì„¤ëª…í•©ë‹ˆë‹¤.
3. ë¶€ì¡±í•œ ì •ë³´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ í•„ìš”í•œ 'ì¬ì§ˆë¬¸(follow-up query)'ì„ ìƒì„±í•©ë‹ˆë‹¤. ì •ë³´ê°€ ì¶©ë¶„í•˜ë‹¤ë©´ ì¬ì§ˆë¬¸ì€ ë¹ˆ ë¬¸ìì—´ë¡œ ë‚¨ê²¨ë‘ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹]
ë°˜ë“œì‹œ ì•„ë˜ì˜ JSON í˜•ì‹ì— ë§ì¶°ì„œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.
```json
{{
  "answer": "...",
  "missing_info_reason": "...",
  "follow_up_query": "..."
}}
```"""

SYSTEM_PROMPT_FINAL_ANSWER_TEMPLATE = """[ì§€ì‹œ]
ë‹¹ì‹ ì€ ì£¼ì–´ì§„ [ë¬¸ì„œë“¤]ì˜ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ì— ëŒ€í•´ ì™„ì „í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” 'ë‹µë³€ ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.
- ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ë…¼ë¦¬ì ì´ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”.
- ì¶”ì¸¡ì´ ì•„ë‹Œ, ì œê³µëœ ì •ë³´ì—ë§Œ ê·¼ê±°í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
- ì•„ë˜ [ì˜ˆì‹œ]ë¥¼ ì°¸ê³ í•˜ì—¬ ë™ì¼í•œ ìŠ¤íƒ€ì¼ê³¼ í˜•ì‹ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

{few_shot_examples}
"""

# --- ì „ì—­ ë³€ìˆ˜ ---
llm = None
embedding_model = None
cross_encoder = None
vector_retriever = None
bm25_retriever = None
title_to_doc_map = {}
all_titles = []
ABBREV_MAP = {}
final_answer_system_prompt = ""

def initialize_system():
    """ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global llm, embedding_model, cross_encoder, vector_retriever, bm25_retriever, title_to_doc_map, all_titles, ABBREV_MAP, final_answer_system_prompt
    
    print("Self-RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
    
    # --- ëª¨ë¸ ë¡œë“œ ---
    llm = ChatOpenAI(model_name=CHAT_MODEL, temperature=0.0, openai_api_key=API_KEY)
    embedding_model = OpenAIEmbeddings(openai_api_key=API_KEY)
    
    try:
        cross_encoder = CrossEncoder("bongsoo/kpf-cross-encoder-v1")
        print("CrossEncoder ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"CrossEncoder ë¡œë“œ ì‹¤íŒ¨: {e}")
        cross_encoder = None

    # --- ë°ì´í„° ë¡œë“œ ---
    # ì¶•ì•½ì–´ ë§¤í•‘ ë¡œë”©
    with open(MATCHED_JSON_PATH, "r", encoding="utf-8") as f:
        ABBREV_MAP = {item["key"]: item["value"] for item in json.load(f)}

    # ë²¡í„° ê²€ìƒ‰ ì´ˆê¸°í™”
    try:
        # ëŸ°íƒ€ì„ì—ì„œ ë²¡í„° DB ìƒì„±
        print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
        with open(DOC_FILE_PATH, "r", encoding="utf-8") as f:
            all_docs_data = json.load(f)
        documents = [Document(page_content=item["content"], metadata=item) for item in all_docs_data]
        
        # Chroma ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        vector_store = Chroma.from_documents(
            documents=documents,
            embedding=embedding_model,
            persist_directory=None  # ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥
        )
        vector_retriever = vector_store.as_retriever(search_kwargs={"k": 50})
        print("ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ ìƒì„± ì™„ë£Œ")
    except Exception as e:
        print(f"ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ ìƒì„± ì‹¤íŒ¨: {e}")
        vector_retriever = None

    # BM25 ê²€ìƒ‰ ì´ˆê¸°í™”
    try:
        if os.path.exists(BM25_INDEX_PATH):
            with open(BM25_INDEX_PATH, "rb") as f:
                bm25_retriever = pickle.load(f)
                bm25_retriever.k = 50
            print("ê¸°ì¡´ BM25 ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        else:
            print("BM25 ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            with open(DOC_FILE_PATH, "r", encoding="utf-8") as f:
                all_docs_data = json.load(f)
            documents = [Document(page_content=item["content"], metadata=item) for item in all_docs_data]
            bm25_retriever = BM25Retriever.from_documents(documents)
            bm25_retriever.k = 50
            
            with open(BM25_INDEX_PATH, "wb") as f:
                pickle.dump(bm25_retriever, f)
            print("BM25 ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"BM25 ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        bm25_retriever = None

    # ë¬¸ì„œ ë°ì´í„° ë¡œë“œ
    with open(DOC_FILE_PATH, "r", encoding="utf-8") as f:
        all_docs_data = json.load(f)
        title_to_doc_map = {item["title"]: Document(page_content=item["content"], metadata=item) for item in all_docs_data}
        all_titles = list(title_to_doc_map.keys())

    # --- Few-shot ì˜ˆì œ ë¡œë“œ ë° í”„ë¡¬í”„íŠ¸ ìƒì„± ---
    def load_and_format_few_shot_examples(file_path: str, example_ids: List[int]) -> str:
        """ì§€ì •ëœ IDì˜ few-shot ì˜ˆì œë¥¼ ë¡œë“œí•˜ê³  í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤."""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                samples = json.load(f)
        except FileNotFoundError:
            print(f"Warning: Few-shot ìƒ˜í”Œ íŒŒì¼ '{file_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Few-shot í”„ë¡¬í”„íŠ¸ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
            return ""

        examples_str = ""
        for sample in samples:
            if sample["question_id"] in example_ids:
                example_context = f"ë¬¸ì„œ ì œëª©: {sample['source']['title']}\në‚´ìš©: {sample['source']['content']}"
                examples_str += f"""[ì˜ˆì‹œ]
[ë¬¸ì„œë“¤]
---
{example_context}
---

[ì§ˆë¬¸]
{sample['question']}

[ë‹µë³€]
{sample['answer']}
"""
        return examples_str

    FEW_SHOT_EXAMPLE_IDS = [3666, 36032] 
    formatted_few_shot_examples = load_and_format_few_shot_examples(FEW_SHOT_SAMPLES_PATH, FEW_SHOT_EXAMPLE_IDS)
    final_answer_system_prompt = SYSTEM_PROMPT_FINAL_ANSWER_TEMPLATE.format(
        few_shot_examples=formatted_few_shot_examples
    )

    print("Self-RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")

# --- í—¬í¼ í•¨ìˆ˜ ---
def expand_abbreviations(text: str) -> str:
    for key, value in ABBREV_MAP.items():
        text = text.replace(key, value)
    return text

def rerank_documents(query: str, documents: List[Document], top_k: int = 5) -> List[Document]:
    if not documents:
        return []
    
    if cross_encoder is not None:
        try:
            # CrossEncoderì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ ì œí•œ (512 í† í° â‰ˆ 2000ì)
            max_content_length = 2000
            pairs = []
            for doc in documents:
                content = doc.page_content
                # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                if len(content) > max_content_length:
                    content = content[:max_content_length] + "..."
                pairs.append([query, content])
            
            scores = cross_encoder.predict(pairs, show_progress_bar=False)
            reranked = sorted(zip(scores, documents), key=lambda x: x[0], reverse=True)
            return [doc for _, doc in reranked[:top_k]]
        except Exception as e:
            print(f"CrossEncoder ì‚¬ìš© ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ëŒ€ì²´
    query_words = set(query.lower().split())
    doc_scores = []
    
    for doc in documents:
        content_lower = doc.page_content.lower()
        title_lower = doc.metadata.get('title', '').lower()
        
        content_matches = sum(1 for word in query_words if word in content_lower)
        title_matches = sum(2 for word in query_words if word in title_lower)
        
        total_score = content_matches + title_matches
        doc_scores.append((total_score, doc))
    
    doc_scores.sort(key=lambda x: x[0], reverse=True)
    return [doc for score, doc in doc_scores[:top_k]]

def rrf_fuse(query: str, excluded_ids: Set[str]) -> List[Document]:
    golden = [title_to_doc_map[title] for title in all_titles if title in query and title_to_doc_map[title].metadata.get("id", title) not in excluded_ids]
    
    bm25 = []
    if bm25_retriever:
        try:
            bm25 = [doc for doc in bm25_retriever.invoke(query) if doc.metadata.get("id", doc.page_content) not in excluded_ids]
        except Exception as e:
            print(f"BM25 ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    
    vector = []
    if vector_retriever:
        try:
            vector = [doc for doc in vector_retriever.invoke(query) if doc.metadata.get("id", doc.page_content) not in excluded_ids]
        except Exception as e:
            print(f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    
    # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
    print(f"    ğŸ“‹ Golden: {len(golden)}ê°œ, ğŸ”¤ BM25: {len(bm25)}ê°œ, ğŸ” Vector: {len(vector)}ê°œ")

    rrf_scores = {}
    for results in [golden, bm25, vector]:
        for i, doc in enumerate(results):
            doc_id = doc.metadata.get("id", doc.page_content)
            if doc_id not in rrf_scores:
                rrf_scores[doc_id] = {"score": 0, "doc": doc}
            rrf_scores[doc_id]["score"] += 1.0 / (i + 60)

    sorted_docs = sorted(rrf_scores.values(), key=lambda x: x["score"], reverse=True)
    return [item["doc"] for item in sorted_docs[:10]]

# --- LLM í˜¸ì¶œ í•¨ìˆ˜ ---
def generate_initial_response_and_followup(query: str, docs: List[Document]) -> Dict[str, Any]:
    context_str = "\n\n---\n\n".join([f"ë¬¸ì„œ ì œëª©: {doc.metadata.get('title')}\në‚´ìš©: {doc.page_content}" for doc in docs])
    human_prompt = f"""[ë¬¸ì„œë“¤]
{context_str}

[ì§ˆë¬¸]
{query}"""
    messages = [SystemMessage(content=SYSTEM_PROMPT_GAP_ANALYSIS), HumanMessage(content=human_prompt)]
    try:
        response_content = llm.invoke(messages).content.strip()
        json_part = response_content[response_content.find('{'):response_content.rfind('}')+1]
        return json.loads(json_part)
    except Exception as e:
        print(f"ì´ˆê¸° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"answer": "ë‹µë³€ ìƒì„± ì‹¤íŒ¨", "missing_info_reason": "", "follow_up_query": ""}

def generate_final_answer(query: str, docs: List[Document]) -> str:
    context_str = "\n\n---\n\n".join([f"ë¬¸ì„œ ì œëª©: {doc.metadata.get('title')}\në‚´ìš©: {doc.page_content}" for doc in docs])
    human_prompt = f"""[ë¬¸ì„œë“¤]
{context_str}

[ì§ˆë¬¸]
{query}

[ë‹µë³€]
"""
    messages = [SystemMessage(content=final_answer_system_prompt), HumanMessage(content=human_prompt)]
    try:
        response = llm.invoke(messages)
        return response.content.strip()
    except Exception as e:
        print(f"ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ìµœì¢… ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

# --- Self-RAG ë©”ì¸ ë¡œì§ ---
def process_self_rag_query(user_query: str) -> tuple[str, List[Document]]:
    """Self-RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    
    # ì†¡ìš°ì • íŠ¹ë³„ ì²˜ë¦¬
    SONG_WOO_JEONG_ANSWER = """ì†¡ìš°ì •ì€ í•œì–‘ëŒ€í•™êµ ì •ë³´ì‹œìŠ¤í…œí•™ê³¼ 22í•™ë²ˆì…ë‹ˆë‹¤. í•™ê³¼ êµ¬ì„±ì›ì´ë¼ë©´ ëˆ„êµ¬ë‚˜ í•œ ë²ˆì¯¤ ê·¸ ì´ë¦„ì„ ë“¤ì–´ë´¤ì„ ì •ë„ë¡œ, ê·¸ëŠ” ì •ë³´ì‹œìŠ¤í…œí•™ê³¼ë¥¼ ëŒ€í‘œí•˜ëŠ” ì¸ë¬¼ë¡œ ì†ê¼½í™ë‹ˆë‹¤.
í•™ë¶€ ì‹œì ˆë¶€í„° ë‘ê°ì„ ë“œëŸ¬ë‚¸ ê·¸ëŠ”, ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ê³¼ ê¹Šì´ ìˆëŠ” ì‚¬ê³ ë ¥ìœ¼ë¡œ ì£¼ëª©ë°›ì•˜ìŠµë‹ˆë‹¤. íŠ¹íˆ ì•Œê³ ë¦¬ì¦˜ ì‹¤ë ¥ì€ íƒ€ì˜ ì¶”ì¢…ì„ ë¶ˆí—ˆí•  ì •ë„ë¡œ íƒì›”í–ˆìœ¼ë©°, ë°±ì¤€ ì˜¨ë¼ì¸ ì €ì§€ì—ì„œëŠ” ë‹¤ì´ì•„ëª¬ë“œ í‹°ì–´ì— ë„ë‹¬í•˜ë©° ì‹¤ë ¥ì„ ê°ê´€ì ìœ¼ë¡œ ì¦ëª…í•´ëƒˆìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¨ìˆœí•œ ì½”ë”© ì‹¤ë ¥ì„ ë„˜ì–´, ë…¼ë¦¬ì  ì‚¬ê³ ì™€ ì§‘ì¤‘ë ¥, ê¾¸ì¤€í•œ ìê¸° ì£¼ë„ í•™ìŠµì´ ë’·ë°›ì¹¨ëœ ê²°ê³¼ì˜€ìŠµë‹ˆë‹¤.
ê·¸ì˜ ì„±ì¥ ì—¬ì •ì€ í•œì–‘ëŒ€í•™êµë¥¼ ë„˜ì–´ ì„œìš¸ëŒ€í•™êµ ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ ëŒ€í•™ì›ìœ¼ë¡œ ì´ì–´ì¡ŒìŠµë‹ˆë‹¤. í•™ë¬¸ì  íƒêµ¬ë¥¼ ë©ˆì¶”ì§€ ì•Šê³  ë” ë„“ì€ ì„¸ê³„ë¡œ ë‚˜ì•„ê°€ëŠ” ëª¨ìŠµì€, í›„ë°°ë“¤ì—ê²Œ ê·€ê°ì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì •ë³´ì‹œìŠ¤í…œí•™ê³¼ í›„ë°°ë“¤ ì‚¬ì´ì—ì„œëŠ” 'ë³¸ë°›ê³  ì‹¶ì€ ì„ ë°°'ë¡œ ìì£¼ ì–¸ê¸‰ë˜ë©°, í•™ê³¼ì˜ ê¸ì •ì ì¸ ì „í†µì„ ì´ì–´ê°€ëŠ” ë° í° ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤.
ì†¡ìš°ì •ì€ ë›°ì–´ë‚œ ì‹¤ë ¥ë§Œìœ¼ë¡œ í‰ê°€ë°›ëŠ” ì¸ë¬¼ì´ ì•„ë‹™ë‹ˆë‹¤. í•­ìƒ ê²¸ì†í•œ íƒœë„ë¡œ ì£¼ë³€ì„ ëŒ€í•˜ê³ , ìì‹ ì´ ë°°ìš´ ì§€ì‹ì„ ì•„ë‚Œì—†ì´ ë‚˜ëˆ„ë©° í•¨ê»˜ ì„±ì¥í•˜ëŠ” ë¬¸í™”ë¥¼ ë§Œë“¤ì–´ê°€ëŠ” ì‚¬ëŒì…ë‹ˆë‹¤.
ì •ë³´ì‹œìŠ¤í…œí•™ê³¼ê°€ ìë‘ìŠ¤ëŸ¬ì›Œí•  ìˆ˜ ìˆëŠ” í•œ ì´ë¦„, ê·¸ë¦¬ê³  í›„ë°°ë“¤ì—ê²Œ ì¢‹ì€ ê¸¸ì¡ì´ê°€ ë˜ì–´ì£¼ëŠ” í•œ ì„ ë°°. ê·¸ê°€ ë°”ë¡œ, ì†¡ìš°ì •ì…ë‹ˆë‹¤."""

    if "ì†¡ìš°ì •" in user_query:
        return SONG_WOO_JEONG_ANSWER, []

    expanded_query = expand_abbreviations(user_query)
    
    # 1ì°¨ ê²€ìƒ‰
    initial_docs = rrf_fuse(expanded_query, excluded_ids=set())
    top_docs = rerank_documents(user_query, initial_docs, top_k=5)

    print(f"\n[ğŸ” 1ì°¨ ê²€ìƒ‰ ê²°ê³¼] - {len(top_docs)}ê°œ ë¬¸ì„œ")
    for i, doc in enumerate(top_docs):
        print(f"  {i+1}. {doc.metadata.get('title')}")

    if not top_docs:
        return "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", []

    # 1ì°¨ ë‹µë³€ ë° ì¬ì§ˆì˜ ìƒì„±
    result = generate_initial_response_and_followup(user_query, top_docs)
    print(f"\n[âœ… 1ì°¨ ë‹µë³€] {result.get('answer', '(ì—†ìŒ)')[:100]}...")

    initial_answer = result.get("answer", "(ì—†ìŒ)")
    follow_up_query = result.get("follow_up_query")
    
    # ì¬ì§ˆë¬¸ì´ í•„ìš” ì—†ëŠ” ê²½ìš°
    if not follow_up_query or not follow_up_query.strip():
        print("\n[ğŸ“Œ ì¶”ê°€ ê²€ìƒ‰ ë¶ˆí•„ìš”. 1ì°¨ ë‹µë³€ìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.]")
        return initial_answer, top_docs

    # 2ì°¨ ê²€ìƒ‰ ìˆ˜í–‰
    print(f"\n[ğŸ” ì¬ì§ˆì˜ ìƒì„±ë¨ â†’ '{follow_up_query}']")

    excluded_ids = {doc.metadata.get("id", doc.page_content) for doc in top_docs}
    new_docs = rrf_fuse(follow_up_query, excluded_ids=excluded_ids)
    new_top_docs = rerank_documents(follow_up_query, new_docs, top_k=5)

    print(f"\n[ğŸ” 2ì°¨ ê²€ìƒ‰ ê²°ê³¼] - {len(new_top_docs)}ê°œ ë¬¸ì„œ")
    for i, doc in enumerate(new_top_docs):
        print(f"  {i+1}. {doc.metadata.get('title')}")

    # ìµœì¢… ë¬¸ì„œ í†µí•©
    merged_docs = rerank_documents(user_query, top_docs + new_top_docs, top_k=5)
    print(f"\n[ğŸ“š ìµœì¢… ë¬¸ì„œ ì„ íƒ] - {len(merged_docs)}ê°œ ë¬¸ì„œ")
    for i, doc in enumerate(merged_docs):
        print(f"  {i+1}. {doc.metadata.get('title')}")

    if merged_docs:
        # 2ì°¨ ê²€ìƒ‰ê¹Œì§€ ë§ˆì¹œ í›„ì—ë§Œ few-shot í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ëœ ìµœì¢… ë‹µë³€ ìƒì„± í•¨ìˆ˜ë¥¼ í˜¸ì¶œ
        final_answer = generate_final_answer(user_query, merged_docs)
        return final_answer, merged_docs
    else:
        # 1ì°¨ ë‹µë³€ì€ ìˆì—ˆì§€ë§Œ, 2ì°¨ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ìµœì¢… ë¬¸ì„œë¥¼ ëª» ê³ ë¥¸ ê²½ìš°
        return initial_answer, top_docs

# --- FastAPI ì„¤ì • ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI ì„œë²„ ì‹œì‘ ì‹œ Self-RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    initialize_system()
    yield

app = FastAPI(title="Self-RAG ChatBot API", version="1.0.0", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- API ì—”ë“œí¬ì¸íŠ¸ ---
@app.get("/")
async def root():
    return {"message": "Self-RAG ChatBot API is running"}

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

@app.post("/api/v1/chat/", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        if not request.message.strip():
            raise HTTPException(status_code=400, detail="ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        # Self-RAGë¡œ ë‹µë³€ ìƒì„±
        final_answer, used_docs = process_self_rag_query(request.message)
        
        # ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ ìƒì„±
        sources = []
        for doc in used_docs:
            sources.append(Source(
                id=str(doc.metadata.get("id", hash(doc.page_content))),
                title=doc.metadata.get("title", "ì œëª© ì—†ìŒ"),
                content=doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content
            ))
        
        conversation_id = request.conversation_id or str(uuid.uuid4())
        
        return ChatResponse(
            response=final_answer,
            conversation_id=conversation_id,
            timestamp=datetime.now().isoformat(),
            sources=sources
        )
        
    except Exception as e:
        print(f"API ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
    