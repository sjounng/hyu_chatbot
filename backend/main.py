import os
import json
import pickle
import random
import logging
from collections import OrderedDict
from typing import List, Optional, Dict, Any
from datetime import datetime
import uuid

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
from difflib import SequenceMatcher
from contextlib import asynccontextmanager

# LangChain imports (ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸)
from langchain_chroma import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=getattr(logging, log_level),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('app.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# API í‚¤ ë° ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY")

# --- ì„¤ì • ë° ë¬¸ì„œ ë¡œë”© ---
DATA_DIR = os.getenv("DATA_DIR", "./data")
VECTOR_DB_PATH = os.path.join(DATA_DIR, "chroma_db_hyu")
BM25_INDEX_PATH = os.path.join(DATA_DIR, "bm25_index.pkl")
DOC_FILE_PATH = os.path.join(DATA_DIR, "document.json")
QA_FILE_PATH = os.path.join(DATA_DIR, "question_sample.json")

# OpenAI API í‚¤ ê²€ì¦
if not api_key:
    raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ì „ì—­ ë³€ìˆ˜ë¡œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
embedding_model = None
vector_retriever = None
bm25_retriever = None
hybrid_retriever = None
title_to_doc_map = {}
all_titles = []
qa_samples = []

def initialize_search_system():
    """ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global embedding_model, vector_retriever, bm25_retriever, hybrid_retriever, title_to_doc_map, all_titles, qa_samples
    
    logger.info("1. ê²€ìƒ‰ ì‹œìŠ¤í…œ ë° ì „ì²´ ë¬¸ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
    
    try:
        embedding_model = OpenAIEmbeddings(openai_api_key=api_key)
        vector_retriever = Chroma(
            persist_directory=VECTOR_DB_PATH, 
            embedding_function=embedding_model
        ).as_retriever(search_kwargs={"k": 10})
        
        with open(BM25_INDEX_PATH, "rb") as f:
            bm25_retriever = pickle.load(f)
        
        hybrid_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, vector_retriever], 
            weights=[0.5, 0.5]
        )
        
        with open(DOC_FILE_PATH, "r", encoding="utf-8") as f:
            all_docs_data = json.load(f)
        
        title_to_doc_map = {
            item["title"]: Document(page_content=item["content"], metadata=item)
            for item in all_docs_data
        }
        all_titles = list(title_to_doc_map.keys())
        
        with open(QA_FILE_PATH, "r", encoding="utf-8") as f:
            qa_samples = json.load(f)
        
        logger.info("   -> ë¡œë“œ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"   -> ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

# Pydantic ëª¨ë¸
class ChatRequest(BaseModel):
    message: str
    conversation_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    conversation_id: str
    timestamp: datetime
    sources: List[Dict[str, Any]] = []

# RAG ë‹µë³€ ìƒì„± í•¨ìˆ˜
def get_final_response(original_query: str):
    """GitHub ì €ì¥ì†Œì˜ RAG ë¡œì§ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    logger.info("\n[ë‹¨ê³„ 1: AIë¥¼ ì´ìš©í•œ í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„±]")

    REWRITE_PROMPT = """[ì§€ì‹œ]
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬, ì •ë³´ ê²€ìƒ‰ì— ë” ì í•©í•œ ëª…í™•í•˜ê³  ìƒì„¸í•œ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ [ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸]ì„ 'í•œì–‘ëŒ€í•™êµ' ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ëŠ”ë‹¤ëŠ” ë§¥ë½ì— ë§ê²Œ, ì™„ì „í•œ ë¬¸ì¥ì˜ ìƒì„¸í•œ ì§ˆë¬¸ìœ¼ë¡œ í•œ ë¬¸ì¥ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ë¶™ì´ì§€ ë§ˆì„¸ìš”.

[ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸]
{user_query}

[ì¬êµ¬ì„±ëœ ì§ˆë¬¸]"""

    llm = ChatOpenAI(model_name="gpt-4o", temperature=0, openai_api_key=api_key)
    try:
        formatted_prompt = REWRITE_PROMPT.format(user_query=original_query)
        rewritten_query = llm.invoke(formatted_prompt).content.strip()
        logger.info(f"   -> ì›ë³¸ ì§ˆë¬¸: '{original_query}'")
        logger.info(f"   -> AIê°€ ì¬êµ¬ì„±í•œ ì§ˆë¬¸: '{rewritten_query}'")
    except Exception as e:
        logger.warning(f"   -> í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„± ì‹¤íŒ¨: {e}. ì›ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        rewritten_query = original_query

    # --- ì¬êµ¬ì„±ëœ ì§ˆë¬¸ ê¸°ë°˜ ê²€ìƒ‰ ---
    logger.info("\n[ë‹¨ê³„ 2: í™•ì •ì  Title ê²€ìƒ‰ ì‹œì‘]")

    def is_title_similar(query: str, title: str, threshold: float = 0.85) -> bool:
        return SequenceMatcher(None, query, title).ratio() >= threshold

    query_no_space = rewritten_query.replace(" ", "")
    golden_docs = []
    for title in all_titles:
        title_no_space = title.replace(" ", "")
        if (
            title in rewritten_query
            or title_no_space in query_no_space
            or is_title_similar(query_no_space, title_no_space)
        ):
            golden_docs.append(title_to_doc_map[title])
    if golden_docs:
        logger.info(
            f"   -> 'í™©ê¸ˆ í‹°ì¼“' ë°œê²¬! Title ì¼ì¹˜ ë¬¸ì„œ: {[doc.metadata['title'] for doc in golden_docs]}"
        )

    logger.info("[ë‹¨ê³„ 3: ë³´ì¡° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘]")
    hybrid_docs = hybrid_retriever.invoke(rewritten_query)
    if not hybrid_docs:
        logger.info("   -> í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, BM25 ë‹¨ë… ê²€ìƒ‰ ì‹œë„")
        try:
            hybrid_docs = bm25_retriever.invoke(rewritten_query)
        except Exception as e:
            logger.error(f"   -> BM25 fallback ì‹¤íŒ¨: {e}")
            hybrid_docs = []

    logger.info("[ë‹¨ê³„ 4: ê²°ê³¼ ì¢…í•© ë° ì •ì œ]")
    combined_docs_dict = OrderedDict()
    for doc in golden_docs:
        combined_docs_dict[doc.metadata["id"]] = doc
    for doc in hybrid_docs:
        if doc.metadata["id"] not in combined_docs_dict:
            combined_docs_dict[doc.metadata["id"]] = doc
    final_retrieved_docs = list(combined_docs_dict.values())[:7]

    if not final_retrieved_docs:
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []

    logger.info(f"   -> ìµœì¢…ì ìœ¼ë¡œ {len(final_retrieved_docs)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì„ ë³„í–ˆìŠµë‹ˆë‹¤.")

    # 5. ìµœì¢… GPT ë‹µë³€ ìƒì„±
    context_str = "\n\n---\n\n".join(
        [
            f"ë¬¸ì„œ ì œëª©: {doc.metadata.get('title')}\në‚´ìš©: {doc.page_content}"
            for doc in final_retrieved_docs
        ]
    )
    source_info = [doc.metadata for doc in final_retrieved_docs]

    # Few-shot ì˜ˆì‹œ ìƒì„±
    few_shot_examples = random.sample(qa_samples, 2)
    few_shot_prompt_part = "\n".join(
        [
            f"ì˜ˆì‹œ ì§ˆë¬¸: {ex['question']}\nì˜ˆì‹œ ë‹µë³€: {ex['answer']}"
            for ex in few_shot_examples
        ]
    )

    rag_prompt = f"""[ì§€ì‹œ]
ë‹¹ì‹ ì€ ì—¬ëŸ¬ ë¬¸ì„œ ì¡°ê°ì„ ì¢…í•©í•˜ì—¬ í•˜ë‚˜ì˜ ì™„ì„±ëœ ê¸€ë¡œ ì¬êµ¬ì„±í•˜ëŠ” 'ê¸€ì“°ê¸° ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ [ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©]ì„ ë°”íƒ•ìœ¼ë¡œ, ì•„ë˜ [ì§ˆë¬¸]ì— ëŒ€í•œ ë‹µë³€ì„ ë§¤ìš° ìƒì„¸í•˜ê³ , ë…¼ë¦¬ì ì´ë©°, ì˜ ë‹¤ë“¬ì–´ì§„ ì„¤ëª…ë¬¸ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”.

[ë‹µë³€ ì˜ˆì‹œ]
{few_shot_prompt_part}
---
[ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©]
{context_str}
---
[ì§ˆë¬¸]
{rewritten_query}
[ë‹µë³€]"""

    logger.info("\n[ë‹¨ê³„ 5: ìµœì¢… ë‹µë³€ ìƒì„± ì‹œì‘]")
    try:
        answer = llm.invoke(rag_prompt).content.strip()
        return answer, source_info
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", []

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
    # ì‹œì‘ ì‹œ
    logger.info("í•œì–‘ëŒ€í•™êµ AI ì±—ë´‡ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    success = initialize_search_system()
    if not success:
        logger.error("âŒ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
    
    yield
    
    # ì¢…ë£Œ ì‹œ (í•„ìš”í•œ ê²½ìš° ì •ë¦¬ ì‘ì—…)
    logger.info("ğŸ”„ ì„œë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="í•œì–‘ëŒ€í•™êµ AI ì±—ë´‡ (GitHub ê¸°ë°˜)",
    description="GitHub ì €ì¥ì†Œ ê¸°ë°˜ì˜ í•œì–‘ëŒ€í•™êµ ì •ë³´ ì œê³µ AI ì±—ë´‡",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ëŒ€í™” ê¸°ë¡ ì €ì¥
conversations = {}

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "í•œì–‘ëŒ€í•™êµ AI ì±—ë´‡ (GitHub ê¸°ë°˜)ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!",
        "version": "1.0.0",
        "docs": "/docs"
    }

@app.post("/api/v1/chat/", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ì±„íŒ… ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        logger.info(f"ìƒˆë¡œìš´ ì±„íŒ… ìš”ì²­: {request.message[:50]}...")
        conversation_id = request.conversation_id or str(uuid.uuid4())
        
        # RAG ì‘ë‹µ ìƒì„±
        answer, sources = get_final_response(request.message)
        
        logger.info(f"ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(answer)} ë¬¸ì, {len(sources)} ê°œ ì†ŒìŠ¤")
        
        return ChatResponse(
            response=answer,
            conversation_id=conversation_id,
            timestamp=datetime.now(),
            sources=sources
        )
        
    except Exception as e:
        logger.error(f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        
        # ìš”ì²­ì´ ì·¨ì†Œëœ ê²½ìš°
        if "cancelled" in str(e).lower() or "abort" in str(e).lower():
            raise HTTPException(status_code=499, detail="Client Closed Request")
        
        # OpenAI API ê´€ë ¨ ì˜¤ë¥˜
        if "openai" in str(e).lower() or "api" in str(e).lower():
            raise HTTPException(status_code=503, detail="AI ì„œë¹„ìŠ¤ ì¼ì‹œ ë¶ˆê°€. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        
        # ì¼ë°˜ì ì¸ ì„œë²„ ì˜¤ë¥˜
        raise HTTPException(status_code=500, detail="ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy", 
        "message": "ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.",
        "search_system_loaded": hybrid_retriever is not None
    }

if __name__ == "__main__":
    import uvicorn
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host=host, port=port)
    