import os
import json
import pickle
import random
import logging
from collections import OrderedDict
from typing import List, Optional, Dict, Any
from datetime import datetime
import uuid

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
from difflib import SequenceMatcher
from contextlib import asynccontextmanager

# LangChain imports (ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸)
from langchain_community.vectorstores import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=getattr(logging, log_level),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('app.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# API í‚¤ ë° ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY")

# --- ì„¤ì • ë° ë¬¸ì„œ ë¡œë”© ---
DATA_DIR = os.getenv("DATA_DIR", "./data")
VECTOR_DB_PATH = os.path.join(DATA_DIR, "chroma_db_hyu")
BM25_INDEX_PATH = os.path.join(DATA_DIR, "bm25_index.pkl")
DOC_FILE_PATH = os.path.join(DATA_DIR, "document.json")
QA_FILE_PATH = os.path.join(DATA_DIR, "question_sample.json")

# OpenAI API í‚¤ ê²€ì¦
if not api_key:
    raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ì „ì—­ ë³€ìˆ˜ë¡œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
embedding_model = None
vector_retriever = None
bm25_retriever = None
hybrid_retriever = None
title_to_doc_map = {}
all_titles = []
qa_samples = []

def initialize_search_system():
    """ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global embedding_model, vector_retriever, bm25_retriever, hybrid_retriever, title_to_doc_map, all_titles, qa_samples
    
    logger.info("1. ê²€ìƒ‰ ì‹œìŠ¤í…œ ë° ì „ì²´ ë¬¸ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
    
    try:
        embedding_model = OpenAIEmbeddings(openai_api_key=api_key)
        vector_retriever = Chroma(
            persist_directory=VECTOR_DB_PATH, 
            embedding_function=embedding_model
        ).as_retriever(search_kwargs={"k": 10})
        
        with open(BM25_INDEX_PATH, "rb") as f:
            bm25_retriever = pickle.load(f)
        
        hybrid_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, vector_retriever], 
            weights=[0.5, 0.5]
        )
        
        with open(DOC_FILE_PATH, "r", encoding="utf-8") as f:
            all_docs_data = json.load(f)
        
        title_to_doc_map = {
            item["title"]: Document(page_content=item["content"], metadata=item)
            for item in all_docs_data
        }
        all_titles = list(title_to_doc_map.keys())
        
        with open(QA_FILE_PATH, "r", encoding="utf-8") as f:
            qa_samples = json.load(f)
        
        logger.info("   -> ë¡œë“œ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"   -> ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

# Pydantic ëª¨ë¸
class ChatRequest(BaseModel):
    message: str
    conversation_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    conversation_id: str
    timestamp: datetime
    sources: List[Dict[str, Any]] = []

# RAG ë‹µë³€ ìƒì„± í•¨ìˆ˜ (temp.pyì˜ ê°œì„ ëœ ë¡œì§ ì‚¬ìš©)
def get_final_response(query: str):
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„, ë‹¤ì¤‘ ê²€ìƒ‰, RRF ìœµí•©, ìµœì¢… ë‹µë³€ ìƒì„±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    logger.info("\n[ë‹¨ê³„ 1: ì´ˆê³ ì† ë™ì‹œ ê²€ìƒ‰ (No-API)]")
    
    # ê²€ìƒ‰ A: í™•ì •ì  Title ê²€ìƒ‰ ('ê³¨ë“  í‹°ì¼“')
    golden_docs = []
    query_no_space = query.replace(" ", "")
    for title in all_titles:
        title_no_space = title.replace(" ", "")
        if title in query or title_no_space in query_no_space:
            golden_docs.append(title_to_doc_map[title])
    if golden_docs:
        logger.info(f"   -> 'ê³¨ë“  í‹°ì¼“' ë°œê²¬: {[doc.metadata['title'] for doc in golden_docs]}")

    # ê²€ìƒ‰ B: BM25 í‚¤ì›Œë“œ ê²€ìƒ‰
    bm25_docs = bm25_retriever.invoke(query)
    logger.info(f"   -> BM25 ê²€ìƒ‰ìœ¼ë¡œ {len(bm25_docs)}ê°œì˜ í›„ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    # ê²€ìƒ‰ C: ë²¡í„° ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
    # ì´ ê³¼ì •ì—ì„œ query ì„ë² ë”©ì„ ìœ„í•´ APIê°€ 1íšŒ í˜¸ì¶œë©ë‹ˆë‹¤. (ë§¤ìš° ë¹ ë¦„)
    vector_docs = vector_retriever.invoke(query)
    logger.info(f"   -> ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ {len(vector_docs)}ê°œì˜ í›„ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    # [ë‹¨ê³„ 2: Reciprocal Rank Fusion (RRF)ìœ¼ë¡œ ìˆœìœ„ ìœµí•©]
    logger.info("\n[ë‹¨ê³„ 2: RRFë¥¼ ì´ìš©í•œ ìˆœìœ„ ìœµí•© (No-API)]")
    rrf_scores = {}
    
    # ê° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìˆœíšŒí•˜ë©° RRF ì ìˆ˜ ê³„ì‚°
    # 'ê³¨ë“  í‹°ì¼“'ì€ ê°€ì¥ ë†’ì€ ìš°ì„ ìˆœìœ„ë¥¼ ê°€ì§
    all_search_results = [golden_docs, bm25_docs, vector_docs]
    
    for results in all_search_results:
        for i, doc in enumerate(results):
            doc_id = doc.metadata['id']
            if doc_id not in rrf_scores:
                rrf_scores[doc_id] = {'score': 0, 'doc': doc}
            # k=60ì€ RRFì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°
            rrf_scores[doc_id]['score'] += 1.0 / (i + 60)

    # ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_docs_with_scores = sorted(rrf_scores.values(), key=lambda x: x['score'], reverse=True)
    final_retrieved_docs = [item['doc'] for item in sorted_docs_with_scores][:5]

    if not final_retrieved_docs:
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []
        
    logger.info("\n[ìµœì¢… ì„ ë³„ëœ ë¬¸ì„œ (LLM ì „ë‹¬ìš©)]")
    for i, doc in enumerate(final_retrieved_docs):
        logger.info(f"  {i+1}. [ì¶œì²˜: {doc.metadata.get('title')}]")
    logger.info("-" * 20)
    
    # 3. ìµœì¢… ë‹µë³€ ìƒì„±
    context_str = "\n\n---\n\n".join([f"ë¬¸ì„œ ì œëª©: {doc.metadata.get('title')}\në‚´ìš©: {doc.page_content}" for doc in final_retrieved_docs])
    source_info = [doc.metadata for doc in final_retrieved_docs]
    
    # Few-shot ì˜ˆì‹œ ìƒì„±
    few_shot_examples = random.sample(qa_samples, 2)
    few_shot_prompt_part = "\n\n".join(
        [f"ì˜ˆì‹œ ì§ˆë¬¸: {ex['question']}\nì˜ˆì‹œ ë‹µë³€: {ex['answer']}" for ex in few_shot_examples]
    )
    
    rag_prompt = f"""[ì§€ì‹œ]
ë‹¹ì‹ ì€ ì—¬ëŸ¬ ë¬¸ì„œ ì¡°ê°ì„ ì¢…í•©í•˜ì—¬ í•˜ë‚˜ì˜ ì™„ì„±ëœ ê¸€ë¡œ ì¬êµ¬ì„±í•˜ëŠ” 'ê¸€ì“°ê¸° ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ [ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©]ì„ ë°”íƒ•ìœ¼ë¡œ, ì•„ë˜ [ì§ˆë¬¸]ì— ëŒ€í•œ ë‹µë³€ì„ ë§¤ìš° ìƒì„¸í•˜ê³ , ë…¼ë¦¬ì ì´ë©°, ì˜ ë‹¤ë“¬ì–´ì§„ ì„¤ëª…ë¬¸ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”.
ë‹µë³€ì˜ ìŠ¤íƒ€ì¼ê³¼ í˜•ì‹ì€ ì•„ë˜ [ë‹µë³€ ì˜ˆì‹œ]ë¥¼ ì°¸ê³ í•˜ë˜, ë‚´ìš©ì€ ë°˜ë“œì‹œ [ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©]ì—ë§Œ ê·¼ê±°í•´ì•¼ í•©ë‹ˆë‹¤.

[ë‹µë³€ ì˜ˆì‹œ]
{few_shot_prompt_part}
---
[ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©]
{context_str}
---
[ì§ˆë¬¸]
{query}
[ë‹µë³€]"""
    
    logger.info(f"\n[ë‹¨ê³„ 3: GPT-4o ëª¨ë¸ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± (ìœ ì¼í•œ API í˜¸ì¶œ)]")
    try:
        llm_final = ChatOpenAI(model_name="gpt-4o", temperature=0.2, openai_api_key=api_key)
        answer = llm_final.invoke(rag_prompt).content.strip()
        return answer, source_info
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", []

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
    # ì‹œì‘ ì‹œ
    logger.info("í•œì–‘ëŒ€í•™êµ AI ì±—ë´‡ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    success = initialize_search_system()
    if not success:
        logger.error("âŒ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
    
    yield
    
    # ì¢…ë£Œ ì‹œ (í•„ìš”í•œ ê²½ìš° ì •ë¦¬ ì‘ì—…)
    logger.info("ğŸ”„ ì„œë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="í•œì–‘ëŒ€í•™êµ AI ì±—ë´‡ (GitHub ê¸°ë°˜)",
    description="GitHub ì €ì¥ì†Œ ê¸°ë°˜ì˜ í•œì–‘ëŒ€í•™êµ ì •ë³´ ì œê³µ AI ì±—ë´‡",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ëŒ€í™” ê¸°ë¡ ì €ì¥
conversations = {}

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "í•œì–‘ëŒ€í•™êµ AI ì±—ë´‡ (GitHub ê¸°ë°˜)ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!",
        "version": "1.0.0",
        "docs": "/docs"
    }

@app.post("/api/v1/chat/", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ì±„íŒ… ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        logger.info(f"ìƒˆë¡œìš´ ì±„íŒ… ìš”ì²­: {request.message[:50]}...")
        conversation_id = request.conversation_id or str(uuid.uuid4())
        
        # RAG ì‘ë‹µ ìƒì„±
        answer, sources = get_final_response(request.message)
        
        logger.info(f"ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(answer)} ë¬¸ì, {len(sources)} ê°œ ì†ŒìŠ¤")
        
        return ChatResponse(
            response=answer,
            conversation_id=conversation_id,
            timestamp=datetime.now(),
            sources=sources
        )
        
    except Exception as e:
        logger.error(f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        
        # ìš”ì²­ì´ ì·¨ì†Œëœ ê²½ìš°
        if "cancelled" in str(e).lower() or "abort" in str(e).lower():
            raise HTTPException(status_code=499, detail="Client Closed Request")
        
        # OpenAI API ê´€ë ¨ ì˜¤ë¥˜
        if "openai" in str(e).lower() or "api" in str(e).lower():
            raise HTTPException(status_code=503, detail="AI ì„œë¹„ìŠ¤ ì¼ì‹œ ë¶ˆê°€. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        
        # ì¼ë°˜ì ì¸ ì„œë²„ ì˜¤ë¥˜
        raise HTTPException(status_code=500, detail="ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy", 
        "message": "ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.",
        "search_system_loaded": hybrid_retriever is not None
    }

if __name__ == "__main__":
    import uvicorn
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host=host, port=port)
    