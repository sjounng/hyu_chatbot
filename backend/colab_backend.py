"""
Google Colabì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” í•œì–‘ëŒ€í•™êµ AI ì±—ë´‡ ë°±ì—”ë“œ
"""

import os
import sys
import subprocess
import requests
import json
from datetime import datetime
import uuid
from typing import List, Optional, Dict, Any

# Google Colab í™˜ê²½ í™•ì¸ ë° ì¡°ê±´ë¶€ import
try:
    from google.colab import output
    from IPython.display import display, HTML
    IS_COLAB = True
except ImportError:
    IS_COLAB = False
    # ë¡œì»¬ í™˜ê²½ìš© ëŒ€ì²´ í•¨ìˆ˜
    def display(html):
        print(html)
    def HTML(content):
        return content

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
def install_requirements():
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤."""
    packages = [
        "fastapi==0.104.1",
        "uvicorn[standard]==0.24.0", 
        "pydantic==2.5.0",
        "python-multipart==0.0.6",
        "sentence-transformers==2.2.2",
        "numpy==1.24.3",
        "requests==2.31.0",
        "transformers==4.35.0",
        "torch==2.1.0",
        "accelerate==0.24.0"
    ]
    
    for package in packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ")
        except:
            print(f"âŒ {package} ì„¤ì¹˜ ì‹¤íŒ¨")

# ë°ì´í„° ëª¨ë¸
class ChatRequest:
    def __init__(self, message: str, conversation_id: Optional[str] = None):
        self.message = message
        self.conversation_id = conversation_id

class ChatResponse:
    def __init__(self, response: str, conversation_id: str, timestamp: datetime):
        self.response = response
        self.conversation_id = conversation_id
        self.timestamp = timestamp

class ChatMessage:
    def __init__(self, role: str, content: str, timestamp: datetime, conversation_id: str):
        self.role = role
        self.content = content
        self.timestamp = timestamp
        self.conversation_id = conversation_id

# ì„ë² ë”© ì„œë¹„ìŠ¤
class EmbeddingService:
    def __init__(self):
        try:
            from sentence_transformers import SentenceTransformer
            self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.model = None
    
    def get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.model:
            return [0.0] * 384
        
        try:
            embedding = self.model.encode(text)
            return embedding.tolist()
        except Exception as e:
            print(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return [0.0] * 384

# LLM ì„œë¹„ìŠ¤ (Hugging Face ëª¨ë¸ ì‚¬ìš©)
class LLMService:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        """Hugging Face ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            # í•œêµ­ì–´ì— ìµœì í™”ëœ ëª¨ë¸ (ë” ì‘ì€ ëª¨ë¸ë¡œ ë³€ê²½)
            model_name = "beomi/KoAlpaca-Polyglot-5.8B"
            
            print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True
            )
            
            print("âœ… LLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ëŒ€ì‹  ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    def generate_response(self, prompt: str, max_length: int = 500) -> str:
        """í…ìŠ¤íŠ¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.model or not self.tokenizer:
            return self._simple_response(prompt)
        
        try:
            import torch
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = f"""### ì§ˆë¬¸: {prompt}

### ë‹µë³€:"""
            
            # í† í°í™”
            inputs = self.tokenizer(full_prompt, return_tensors="pt")
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_length=max_length,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # ë””ì½”ë”©
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
            response = response.replace(full_prompt, "").strip()
            
            return response if response else self._simple_response(prompt)
            
        except Exception as e:
            print(f"ìƒì„± ì˜¤ë¥˜: {e}")
            return self._simple_response(prompt)
    
    def _simple_response(self, prompt: str) -> str:
        """ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±ê¸° (ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)"""
        responses = [
            "í•œì–‘ëŒ€í•™êµì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì…¨ë„¤ìš”. ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ë“œë¦´ ìˆ˜ ìˆë„ë¡ ë„ì›€ì´ í•„ìš”í•©ë‹ˆë‹¤.",
            "í•œì–‘ëŒ€í•™êµ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ê³  ê³„ì‹œëŠ”êµ°ìš”. ì–´ë–¤ ë¶€ë¶„ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?",
            "í•œì–‘ëŒ€í•™êµì— ëŒ€í•œ ì§ˆë¬¸ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. í•™ê³¼, ìº í¼ìŠ¤, ì…í•™ ì •ë³´ ë“± êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì„¸ìš”.",
            "í•œì–‘ëŒ€í•™êµ ì •ë³´ë¥¼ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•˜ì‹ ê°€ìš”?"
        ]
        
        import random
        return random.choice(responses)

# ë¬¸ì„œ ì„œë¹„ìŠ¤
class DocumentService:
    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.documents = {}
        self.chunks = {}
        self._load_sample_data()
    
    def _load_sample_data(self):
        """ìƒ˜í”Œ í•œì–‘ëŒ€í•™êµ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        sample_docs = [
            {
                "title": "í•œì–‘ëŒ€í•™êµ ê°œìš”",
                "content": "í•œì–‘ëŒ€í•™êµëŠ” 1939ë…„ì— ì„¤ë¦½ëœ ëŒ€í•œë¯¼êµ­ì˜ ì‚¬ë¦½ëŒ€í•™êµì…ë‹ˆë‹¤. ì„œìš¸íŠ¹ë³„ì‹œ ì„±ë™êµ¬ì— ìœ„ì¹˜í•œ ì„œìš¸ìº í¼ìŠ¤ì™€ ê²½ê¸°ë„ ì•ˆì‚°ì‹œì— ìœ„ì¹˜í•œ ERICAìº í¼ìŠ¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê³µí•™, ì˜í•™, ê²½ì˜í•™ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë³´ì´ê³  ìˆìœ¼ë©°, íŠ¹íˆ ê³µí•™ ë¶„ì•¼ì—ì„œ êµ­ë‚´ ìµœê³  ìˆ˜ì¤€ì˜ êµìœ¡ì„ ì œê³µí•©ë‹ˆë‹¤."
            },
            {
                "title": "í•œì–‘ëŒ€í•™êµ í•™ê³¼",
                "content": "í•œì–‘ëŒ€í•™êµëŠ” ê³µê³¼ëŒ€í•™, ì˜ê³¼ëŒ€í•™, ê²½ì˜ëŒ€í•™, ì‚¬ë²”ëŒ€í•™, ë¬¸ê³¼ëŒ€í•™, ì˜ˆìˆ ì²´ìœ¡ëŒ€í•™, êµ­ì œë¬¸í™”ëŒ€í•™ ë“± ë‹¤ì–‘í•œ ë‹¨ê³¼ëŒ€í•™ì„ ìš´ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì£¼ìš” í•™ê³¼ë¡œëŠ” ê±´ì¶•í•™ê³¼, ê¸°ê³„ê³µí•™ê³¼, ì „ìê³µí•™ê³¼, ì»´í“¨í„°ê³µí•™ê³¼, ì˜í•™ê³¼, ê²½ì˜í•™ê³¼, êµ­ì–´êµ­ë¬¸í•™ê³¼ ë“±ì´ ìˆìŠµë‹ˆë‹¤."
            },
            {
                "title": "í•œì–‘ëŒ€í•™êµ ìº í¼ìŠ¤",
                "content": "í•œì–‘ëŒ€í•™êµëŠ” ì„œìš¸ìº í¼ìŠ¤ì™€ ERICAìº í¼ìŠ¤ ë‘ ê°œì˜ ìº í¼ìŠ¤ë¥¼ ìš´ì˜í•©ë‹ˆë‹¤. ì„œìš¸ìº í¼ìŠ¤ëŠ” ì„œìš¸íŠ¹ë³„ì‹œ ì„±ë™êµ¬ ì™•ì‹­ë¦¬ë¡œì— ìœ„ì¹˜í•˜ë©°, ERICAìº í¼ìŠ¤ëŠ” ê²½ê¸°ë„ ì•ˆì‚°ì‹œ ìƒë¡êµ¬ í•œì–‘ëŒ€í•™ë¡œì— ìœ„ì¹˜í•©ë‹ˆë‹¤. ê° ìº í¼ìŠ¤ëŠ” ë…ë¦½ì ì¸ êµìœ¡ í™˜ê²½ì„ ì œê³µí•˜ë©°, í•™ìƒë“¤ì€ ìì‹ ì˜ ì „ê³µì— ë”°ë¼ ì ì ˆí•œ ìº í¼ìŠ¤ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            }
        ]
        
        for i, doc in enumerate(sample_docs):
            doc_id = str(uuid.uuid4())
            self.documents[doc_id] = {
                "id": doc_id,
                "title": doc["title"],
                "content": doc["content"],
                "created_at": datetime.now()
            }
            
            # ì²­í¬ ìƒì„±
            chunks = self._split_content(doc["content"])
            for j, chunk_text in enumerate(chunks):
                chunk_id = str(uuid.uuid4())
                self.chunks[chunk_id] = {
                    "id": chunk_id,
                    "document_id": doc_id,
                    "content": chunk_text,
                    "embedding": self.embedding_service.get_embedding(chunk_text),
                    "metadata": {"title": doc["title"], "chunk_index": j}
                }
    
    def _split_content(self, content: str) -> List[str]:
        """ë‚´ìš©ì„ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        sentences = content.split('. ')
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) < 200:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def search_similar_chunks(self, query: str, top_k: int = 3) -> List[Dict]:
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ì²­í¬ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        if not self.chunks:
            return []
        
        query_embedding = self.embedding_service.get_embedding(query)
        
        similarities = []
        for chunk in self.chunks.values():
            if chunk["embedding"]:
                similarity = self._cosine_similarity(query_embedding, chunk["embedding"])
                similarities.append((similarity, chunk))
        
        similarities.sort(key=lambda x: x[0], reverse=True)
        return [chunk for _, chunk in similarities[:top_k]]
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        import numpy as np
        
        try:
            vec1 = np.array(vec1)
            vec2 = np.array(vec2)
            
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = np.dot(vec1, vec2) / (norm1 * norm2)
            return float(similarity)
            
        except Exception as e:
            print(f"ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.0
    
    def get_context_for_query(self, query: str) -> str:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        similar_chunks = self.search_similar_chunks(query, top_k=2)
        
        if not similar_chunks:
            return ""
        
        context_parts = []
        for chunk in similar_chunks:
            context_parts.append(f"ì¶œì²˜: {chunk['metadata']['title']}\n{chunk['content']}")
        
        return "\n\n".join(context_parts)

# ì±„íŒ… ì„œë¹„ìŠ¤
class ChatService:
    def __init__(self):
        self.llm_service = LLMService()
        self.document_service = DocumentService()
        self.conversations = {}
    
    def process_message(self, request: ChatRequest) -> ChatResponse:
        """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³  AI ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        
        conversation_id = request.conversation_id or str(uuid.uuid4())
        conversation = self.conversations.get(conversation_id, [])
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        user_message = ChatMessage(
            role="user",
            content=request.message,
            timestamp=datetime.now(),
            conversation_id=conversation_id
        )
        conversation.append(user_message)
        
        # RAG ì‘ë‹µ ìƒì„±
        ai_response = self._generate_rag_response(request.message)
        
        # AI ë©”ì‹œì§€ ì¶”ê°€
        assistant_message = ChatMessage(
            role="assistant",
            content=ai_response,
            timestamp=datetime.now(),
            conversation_id=conversation_id
        )
        conversation.append(assistant_message)
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.conversations[conversation_id] = conversation
        
        return ChatResponse(
            response=ai_response,
            conversation_id=conversation_id,
            timestamp=datetime.now()
        )
    
    def _generate_rag_response(self, user_message: str) -> str:
        """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ AI ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        
        # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        context = self.document_service.get_context_for_query(user_message)
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if context:
            prompt = f"""í•œì–‘ëŒ€í•™êµì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì°¸ê³  ì •ë³´:
{context}

ì§ˆë¬¸: {user_message}

ë‹µë³€:"""
        else:
            prompt = f"""í•œì–‘ëŒ€í•™êµì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {user_message}

ë‹µë³€:"""
        
        # LLM ì‘ë‹µ ìƒì„±
        response = self.llm_service.generate_response(prompt)
        
        return response

# FastAPI ì•±
def create_app():
    """FastAPI ì•±ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    from fastapi import FastAPI, HTTPException
    from fastapi.middleware.cors import CORSMiddleware
    from pydantic import BaseModel
    
    app = FastAPI(
        title="í•œì–‘ëŒ€í•™êµ AI ì±—ë´‡ (Colab)",
        description="Google Colabì—ì„œ ì‹¤í–‰ë˜ëŠ” í•œì–‘ëŒ€í•™êµ ì •ë³´ ì œê³µ AI ì±—ë´‡",
        version="1.0.0"
    )
    
    # CORS ì„¤ì •
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # Pydantic ëª¨ë¸
    class ChatRequestModel(BaseModel):
        message: str
        conversation_id: Optional[str] = None
    
    class ChatResponseModel(BaseModel):
        response: str
        conversation_id: str
        timestamp: datetime
    
    # ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
    chat_service = ChatService()
    
    @app.get("/")
    async def root():
        return {
            "message": "í•œì–‘ëŒ€í•™êµ AI ì±—ë´‡ (Colab)ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!",
            "version": "1.0.0",
            "docs": "/docs"
        }
    
    @app.post("/api/v1/chat/", response_model=ChatResponseModel)
    async def chat(request: ChatRequestModel):
        """ì±„íŒ… ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            chat_request = ChatRequest(
                message=request.message,
                conversation_id=request.conversation_id
            )
            response = chat_service.process_message(chat_request)
            
            return ChatResponseModel(
                response=response.response,
                conversation_id=response.conversation_id,
                timestamp=response.timestamp
            )
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.get("/health")
    async def health_check():
        return {"status": "healthy", "message": "ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤."}
    
    return app

# Colab ì‹¤í–‰ í•¨ìˆ˜
def run_colab_server():
    """Colabì—ì„œ ì„œë²„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    print("ğŸš€ í•œì–‘ëŒ€í•™êµ AI ì±—ë´‡ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # íŒ¨í‚¤ì§€ ì„¤ì¹˜
    print("ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤...")
    install_requirements()
    
    # ì•± ìƒì„±
    app = create_app()
    
    # ngrok ì„¤ì¹˜ ë° ì‹¤í–‰ (ì™¸ë¶€ ì ‘ê·¼ìš©)
    try:
        print("ğŸŒ ngrokì„ ì„¤ì¹˜í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤...")
        subprocess.run(["pip", "install", "pyngrok"], check=True)
        
        from pyngrok import ngrok
        
        # FastAPI ì„œë²„ ì‹œì‘
        import uvicorn
        import threading
        
        def run_server():
            uvicorn.run(app, host="0.0.0.0", port=8000)
        
        server_thread = threading.Thread(target=run_server)
        server_thread.daemon = True
        server_thread.start()
        
        # ì ì‹œ ëŒ€ê¸°
        import time
        time.sleep(3)
        
        # ngrok í„°ë„ ìƒì„±
        public_url = ngrok.connect(8000)
        print(f"âœ… ì„œë²„ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸŒ ê³µê°œ URL: {public_url}")
        print(f"ğŸ“š API ë¬¸ì„œ: {public_url}/docs")
        
        # URLì„ HTMLë¡œ í‘œì‹œ
        display(HTML(f"""
        <div style="background: #f0f8ff; padding: 20px; border-radius: 10px; margin: 20px 0;">
            <h3>ğŸ‰ ì„œë²„ ì‹¤í–‰ ì™„ë£Œ!</h3>
            <p><strong>ê³µê°œ URL:</strong> <a href="{public_url}" target="_blank">{public_url}</a></p>
            <p><strong>API ë¬¸ì„œ:</strong> <a href="{public_url}/docs" target="_blank">{public_url}/docs</a></p>
            <p><strong>í—¬ìŠ¤ ì²´í¬:</strong> <a href="{public_url}/health" target="_blank">{public_url}/health</a></p>
        </div>
        """))
        
        return public_url
        
    except Exception as e:
        print(f"âŒ ngrok ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        print("ë¡œì»¬ì—ì„œë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        return None

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_chat():
    """ì±„íŒ… ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    print("ğŸ§ª ì±„íŒ… ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤...")
    
    chat_service = ChatService()
    
    test_messages = [
        "í•œì–‘ëŒ€í•™êµëŠ” ì–´ë–¤ ëŒ€í•™êµì¸ê°€ìš”?",
        "í•œì–‘ëŒ€í•™êµ í•™ê³¼ëŠ” ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?",
        "í•œì–‘ëŒ€í•™êµ ìº í¼ìŠ¤ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?"
    ]
    
    for message in test_messages:
        print(f"\nğŸ‘¤ ì‚¬ìš©ì: {message}")
        
        request = ChatRequest(message=message)
        response = chat_service.process_message(request)
        
        print(f"ğŸ¤– AI: {response.response}")
        print("-" * 50)

if __name__ == "__main__":
    # Colabì—ì„œ ì‹¤í–‰í•  ë•Œ
    public_url = run_colab_server()
    
    if public_url:
        print(f"\nğŸ¯ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë‹¤ìŒ URLë¡œ ì—°ê²°í•˜ì„¸ìš”: {public_url}")
    else:
        print("\në¡œì»¬ì—ì„œë§Œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.") 