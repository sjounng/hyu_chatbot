import os
import json
import pickle
import random
from collections import OrderedDict
from langchain_community.vectorstores import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from typing import List
from dotenv import load_dotenv

# --- 0. ì„¤ì • ë° ì´ˆê¸°í™” ---
load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")

# ëª¨ë¸ ë° íŒŒì¼ ê²½ë¡œ ì„¤ì •
CHAT_MODEL = 'gpt-4o'
VECTOR_DB_PATH = "./chroma_db_hyu"
BM25_INDEX_PATH = "./bm25_index.pkl"
DOC_FILE_PATH = "hyuwiki_documents_20250621_234549.json"
QA_FILE_PATH = "qa_random_200_samples_20250622_203907.json"

# --- 1. ê²€ìƒ‰ ì‹œìŠ¤í…œ ë¡œë“œ ---
print("1. ê²€ìƒ‰ ì‹œìŠ¤í…œ ë° ì „ì²´ ë¬¸ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
# 1-1. ë²¡í„° ê²€ìƒ‰ê¸° ë¡œë“œ (ì˜ë¯¸ ê¸°ë°˜)
embedding_model = OpenAIEmbeddings(openai_api_key=API_KEY)
vector_retriever = Chroma(
    persist_directory=VECTOR_DB_PATH, embedding_function=embedding_model
).as_retriever(search_kwargs={"k": 20})

# 1-2. BM25 ê²€ìƒ‰ê¸° ë¡œë“œ (í‚¤ì›Œë“œ ê¸°ë°˜)
with open(BM25_INDEX_PATH, "rb") as f:
    bm25_retriever = pickle.load(f)
    bm25_retriever.k = 20

# 1-3. Title ê²€ìƒ‰ì„ ìœ„í•œ ë°ì´í„° ë¡œë“œ
with open(DOC_FILE_PATH, "r", encoding="utf-8") as f:
    all_docs_data = json.load(f)
title_to_doc_map = {item["title"]: Document(page_content=item["content"], metadata=item) for item in all_docs_data}
all_titles = list(title_to_doc_map.keys())
print("   -> ë¡œë“œ ì™„ë£Œ!")

# --- 2. Few-shot ì˜ˆì‹œ ìƒì„± ---
with open(QA_FILE_PATH, "r", encoding="utf-8") as f:
    qa_samples = json.load(f)
few_shot_examples = random.sample(qa_samples, 2)
few_shot_prompt_part = "\n\n".join(
    [f"ì˜ˆì‹œ ì§ˆë¬¸: {ex['question']}\nì˜ˆì‹œ ë‹µë³€: {ex['answer']}" for ex in few_shot_examples]
)


# --- 3. ìµœì¢… RAG ë‹µë³€ ìƒì„± í•¨ìˆ˜ (RRF ì ìš©) ---
def get_final_response(query: str):
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„, ë‹¤ì¤‘ ê²€ìƒ‰, RRF ìœµí•©, ìµœì¢… ë‹µë³€ ìƒì„±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("\n[ë‹¨ê³„ 1: ì´ˆê³ ì† ë™ì‹œ ê²€ìƒ‰ (No-API)]")
    
    # ê²€ìƒ‰ A: í™•ì •ì  Title ê²€ìƒ‰ ('ê³¨ë“  í‹°ì¼“')
    golden_docs = []
    query_no_space = query.replace(" ", "")
    for title in all_titles:
        title_no_space = title.replace(" ", "")
        if title in query or title_no_space in query_no_space:
            golden_docs.append(title_to_doc_map[title])
    if golden_docs:
        print(f"   -> 'ê³¨ë“  í‹°ì¼“' ë°œê²¬: {[doc.metadata['title'] for doc in golden_docs]}")

    # ê²€ìƒ‰ B: BM25 í‚¤ì›Œë“œ ê²€ìƒ‰
    bm25_docs = bm25_retriever.invoke(query)
    print(f"   -> BM25 ê²€ìƒ‰ìœ¼ë¡œ {len(bm25_docs)}ê°œì˜ í›„ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    # ê²€ìƒ‰ C: ë²¡í„° ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
    # ì´ ê³¼ì •ì—ì„œ query ì„ë² ë”©ì„ ìœ„í•´ APIê°€ 1íšŒ í˜¸ì¶œë©ë‹ˆë‹¤. (ë§¤ìš° ë¹ ë¦„)
    vector_docs = vector_retriever.invoke(query)
    print(f"   -> ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ {len(vector_docs)}ê°œì˜ í›„ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    # [ë‹¨ê³„ 2: Reciprocal Rank Fusion (RRF)ìœ¼ë¡œ ìˆœìœ„ ìœµí•©]
    print("\n[ë‹¨ê³„ 2: RRFë¥¼ ì´ìš©í•œ ìˆœìœ„ ìœµí•© (No-API)]")
    rrf_scores = {}
    
    # ê° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìˆœíšŒí•˜ë©° RRF ì ìˆ˜ ê³„ì‚°
    # 'ê³¨ë“  í‹°ì¼“'ì€ ê°€ì¥ ë†’ì€ ìš°ì„ ìˆœìœ„ë¥¼ ê°€ì§
    all_search_results = [golden_docs, bm25_docs, vector_docs]
    
    for results in all_search_results:
        for i, doc in enumerate(results):
            doc_id = doc.metadata['id']
            if doc_id not in rrf_scores:
                rrf_scores[doc_id] = {'score': 0, 'doc': doc}
            # k=60ì€ RRFì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°
            rrf_scores[doc_id]['score'] += 1.0 / (i + 60)

    # ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_docs_with_scores = sorted(rrf_scores.values(), key=lambda x: x['score'], reverse=True)
    final_retrieved_docs = [item['doc'] for item in sorted_docs_with_scores][:5]

    if not final_retrieved_docs:
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []
        
    print("\n[ìµœì¢… ì„ ë³„ëœ ë¬¸ì„œ (LLM ì „ë‹¬ìš©)]")
    for i, doc in enumerate(final_retrieved_docs):
        print(f"  {i+1}. [ì¶œì²˜: {doc.metadata.get('title')}]")
    print("-" * 20)
    
    # 3. ìµœì¢… ë‹µë³€ ìƒì„±
    context_str = "\n\n---\n\n".join([f"ë¬¸ì„œ ì œëª©: {doc.metadata.get('title')}\në‚´ìš©: {doc.page_content}" for doc in final_retrieved_docs])
    source_info = [doc.metadata for doc in final_retrieved_docs]
    
    rag_prompt = f"""[ì§€ì‹œ]
ë‹¹ì‹ ì€ ì—¬ëŸ¬ ë¬¸ì„œ ì¡°ê°ì„ ì¢…í•©í•˜ì—¬ í•˜ë‚˜ì˜ ì™„ì„±ëœ ê¸€ë¡œ ì¬êµ¬ì„±í•˜ëŠ” 'ê¸€ì“°ê¸° ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ [ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©]ì„ ë°”íƒ•ìœ¼ë¡œ, ì•„ë˜ [ì§ˆë¬¸]ì— ëŒ€í•œ ë‹µë³€ì„ ë§¤ìš° ìƒì„¸í•˜ê³ , ë…¼ë¦¬ì ì´ë©°, ì˜ ë‹¤ë“¬ì–´ì§„ ì„¤ëª…ë¬¸ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”.
ë‹µë³€ì˜ ìŠ¤íƒ€ì¼ê³¼ í˜•ì‹ì€ ì•„ë˜ [ë‹µë³€ ì˜ˆì‹œ]ë¥¼ ì°¸ê³ í•˜ë˜, ë‚´ìš©ì€ ë°˜ë“œì‹œ [ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©]ì—ë§Œ ê·¼ê±°í•´ì•¼ í•©ë‹ˆë‹¤.

[ë‹µë³€ ì˜ˆì‹œ]
{few_shot_prompt_part}
---
[ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©]
{context_str}
---
[ì§ˆë¬¸]
{query}
[ë‹µë³€]"""
    
    print(f"\n[ë‹¨ê³„ 3: {CHAT_MODEL} ëª¨ë¸ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± (ìœ ì¼í•œ API í˜¸ì¶œ)]")
    try:
        llm_final = ChatOpenAI(model_name=CHAT_MODEL, temperature=0.2, openai_api_key=API_KEY)
        answer = llm_final.invoke(rag_prompt).content.strip()
        return answer, source_info
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", []

# --- 4. ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
if __name__ == "__main__":
    print("\nì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'ì¢…ë£Œ' ì…ë ¥)")
    while True:
        user_query = input("\nğŸ¤” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
        if user_query.lower() in ['exit', 'ì¢…ë£Œ']:
            print("ğŸ¤– ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."); break
        answer, sources = get_final_response(user_query)
        print("\n" + "="*50)
        print(f"ğŸ¤– ë‹µë³€:\n{answer}")
        if sources:
            unique_sources = list(OrderedDict.fromkeys((src.get('title'), src.get('url')) for src in sources))
            print("\nğŸ“š ì°¸ê³  ìë£Œ:")
            for title, url in unique_sources:
                print(f"  - {title} ({url})")
        print("="*50)
